# -*- coding: utf-8 -*-
"""data_plot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_HyEFHF9eApSqNcR5gxu4JSc6El289Oh
"""

import numpy as np
import matplotlib.pyplot as plt
import os

clean = True

#剔除立群点以便于作图观察
def DataClean(data,label,dataset):
    judges = np.abs(data - data.mean(axis=0)) <= (3 * data.std(axis=0))
    choose = np.full(len(data),True)
    for i in range(len(judges)):
        if any(judges[i] != np.full((10), True)):
            choose[i] = False
    new_data = data[choose]
    
    for i in range(len(new_data)):
        a = list(map(float, new_data[i]))
        a.append(label)
        dataset.append(a)
    
    return new_data

file_train = open("avila-tr.txt")
line = file_train.readline()
train_data = []
train_label = []
train_dataset = []
while line:
    data = line.split(',')
    train_data.append(list(map(float, data[:-1])))
    train_label.append(data[-1])
    
    a = list(map(float, data[:-1]))
    a.append(data[-1][0])
    train_dataset.append(a)
    
    line = file_train.readline()
    
file_train.close()
train_data = np.array(train_data)
train_label = np.array(train_label)

"""### histogram, box plot and scatter plot"""

A = []
B = []
C = []
D = []
E = []
F = []
G = []
H = []
I = []
W = []
X = []
Y = []

A_log = []
B_log = []
C_log = []
D_log = []
E_log = []
F_log = []
G_log = []
H_log = []
I_log = []
W_log = []
X_log = []
Y_log = []
for i in range(len(train_data)):
    if train_label[i][0] == 'A':
        A.append(train_data[i])
        A_log.append(i)
    elif train_label[i][0] == 'B':
        B.append(train_data[i])
        B_log.append(i)
    elif train_label[i][0] == 'C':
        C.append(train_data[i])
        C_log.append(i)
    elif train_label[i][0] == 'D':
        D.append(train_data[i])
        D_log.append(i)
    elif train_label[i][0] == 'E':
        E.append(train_data[i])
        E_log.append(i)
    elif train_label[i][0] == 'F':
        F.append(train_data[i])
        F_log.append(i)
    elif train_label[i][0] == 'G':
        G.append(train_data[i])
        G_log.append(i)
    elif train_label[i][0] == 'H':
        H.append(train_data[i])
        H_log.append(i)
    elif train_label[i][0] == 'I':
        I.append(train_data[i])
        I_log.append(i)
    elif train_label[i][0] == 'W':
        W.append(train_data[i])
        W_log.append(i)
    elif train_label[i][0] == 'X':
        X.append(train_data[i])
        X_log.append(i)
    elif train_label[i][0] == 'Y':
        Y.append(train_data[i])
        Y_log.append(i)


A = np.array(A)
B = np.array(B)
C = np.array(C)
D = np.array(D)
E = np.array(E)
F = np.array(F)
G = np.array(G)
H = np.array(H)
I = np.array(I)
W = np.array(W)
X = np.array(X)
Y = np.array(Y)

new_train_dataset = []
new_A = DataClean(A,'A',new_train_dataset)
new_B = DataClean(B,'B',new_train_dataset)
new_C = DataClean(C,'C',new_train_dataset)
new_D = DataClean(D,'D',new_train_dataset)
new_E = DataClean(E,'E',new_train_dataset)
new_F = DataClean(F,'F',new_train_dataset)
new_G = DataClean(G,'G',new_train_dataset)
new_H = DataClean(H,'H',new_train_dataset)
new_I = DataClean(I,'I',new_train_dataset)
new_W = DataClean(W,'W',new_train_dataset)
new_X = DataClean(X,'X',new_train_dataset)
new_Y = DataClean(Y,'Y',new_train_dataset)

print(len(new_train_dataset))

color = ['gray', 'pink']

def AttributeHist(j,clean=True):
    data_A = A[:, j]
    data_B = B[:, j]
    data_C = C[:, j]
    
    if clean:
        data_A = new_A[:, j]  
        data_B = new_B[:, j]
        data_C = new_C[:, j]
    
    plt.hist([data_A, data_B, data_C], 5, rwidth=0.5, color=['r', 'b', 'y'])
    plt.legend(['A', 'B', 'C'])
    plt.ylabel('number')
    plt.title('Hist of Attribute{}'.format(j))
    plt.show()


def AttributeBoxplot(j):
    data_A = A[:, j]
    data_B = B[:, j]
    data_C = C[:, j]
    
    plt.boxplot([data_A, data_B, data_C], sym='o',  # 异常点形状
                vert=True,  # 是否垂直
                patch_artist=True,  # 上下四分位框是否填充
                meanline=True,  # 是否有均值线
                showmeans=True,
                showbox=True,  # 是否显示箱线
                showfliers=True  # 是否显示异常值
                )
    plt.title('Boxplot of Attribute{}'.format(j))
    plt.xticks([1, 2, 3], ['A', 'B', 'C'])
    plt.show()


def AttributeScatterplot(i,j,clean): 
    data_Ai = A[:, i]
    data_Bi = B[:, i]
    data_Ci = C[:, i]
    data_Aj = A[:, j]
    data_Bj = B[:, j]
    data_Cj = C[:, j]
    if clean:
        data_Ai = new_A[:, i]  
        data_Bi = new_B[:, i]
        data_Ci = new_C[:, i]
        data_Aj = new_A[:, j]  
        data_Bj = new_B[:, j]
        data_Cj = new_C[:, j]
        
    plt.plot(data_Ai,data_Aj, 'o')
    plt.plot(data_Ci,data_Cj, 'o')
    plt.plot(data_Bi,data_Bj, 'o')
    plt.title('Scatterplot of Attribute{},{}'.format(i,j))
    plt.legend(['A', 'C', 'B'])
    plt.show()


print("----------------------histogram------------------------")
print("leave the far outliers")
for i in range(10):
    AttributeHist(i,True) 
print("preserve the far outliers")
for i in range(10):
    AttributeHist(i,False)

print("----------------------box plot------------------------")
for i in range(10):
    AttributeBoxplot(i)
    

print("----------------------scatter plot------------------------")
print("leave the far outliers")
for i in range(10):
    for j in range(10):
        AttributeScatterplot(i,j,clean)

"""### matrix plot"""

import seaborn as sns

def DatasetMatrixplot(dataset):
    # sns.heatmap(dataset,annot=True,vmax=0.8,cmap='RdBu_r',square=True,center=0)
    sns.heatmap(dataset,vmax=4)



A = np.mean(A, axis=0)
B = np.mean(B, axis=0)
C = np.mean(C, axis=0)
D = np.mean(D, axis=0)
E = np.mean(E, axis=0)
F = np.mean(F, axis=0)
G = np.mean(G, axis=0)
H = np.mean(H, axis=0)
I = np.mean(I, axis=0)
W = np.mean(W, axis=0)
X = np.mean(X, axis=0)
Y = np.mean(Y, axis=0)

train_data_new = np.array([A, B, C, D, E, F, G, H, I, W, X, Y])
DatasetMatrixplot(train_data_new)


plt.yticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [
           'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y'])
plt.xlabel('Attribute')
plt.ylabel('class')
plt.show()

"""### pairwise similarity """

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics.pairwise import manhattan_distances


def CosineSimilarity(dataset):
    return cosine_similarity(dataset)


def EulerSimilarity(dataset):
    return euclidean_distances(dataset)


def ManhattanSimilarity(dataset):
    return manhattan_distances(dataset)

print(train_dataset[0][:-1])

cos = 1 - CosineSimilarity([i[:-1] for i in train_dataset])
DatasetMatrixplot(cos)
plt.show()

plt.figure(figsize=(21,21))
cos = 1 - CosineSimilarity([i[:-1] for i in train_dataset])
DatasetMatrixplot(cos)
plt.show()

plt.figure(figsize=(21,21))
euler = EulerSimilarity([i[:-1] for i in train_dataset])
DatasetMatrixplot(euler)
plt.show()

plt.figure(figsize=(21,21))
manhattan = ManhattanSimilarity([i[:-1] for i in train_dataset])
DatasetMatrixplot(manhattan)
plt.show()

"""### parallel coordinates"""

from pandas.plotting import parallel_coordinates
from pandas import DataFrame

df=DataFrame(new_train_dataset, columns=range(1,12))
df

plt.figure()
parallel_coordinates(df, 11)
#lt.legend(train_label)

"""### star coordinates + chernoff faces"""

import numpy as np
import matplotlib.pyplot as plt
#plt.rcParams['font.sans-serif'] = ['SimHei']
def DrawStarCoordinates(j):
    name = ['F1','F2','F3','F4','F5', 'F6','F7','F8','F9','F10']   #标签
    theta = np.linspace(0,2*np.pi,len(name),endpoint=False)    #将圆根据标签的个数等比分
    value = train_data[j]  
    theta = np.concatenate((theta,[theta[0]]))  #闭合
    value = np.concatenate((value,[value[0]]))  #闭合
    name.append('F1')
    
    ax = plt.subplot(111,projection = 'polar',alpha=0)      #构建图例
    ax.plot(theta,value,'m-',lw=1,alpha = 1)    #绘图
    ax.fill(theta,value,'m',alpha = 0)           #填充
    ax.set_thetagrids(theta*180/np.pi,name)         #替换标签
    #ax.set_ylim(0,110)                          #设置极轴的区间
    ax.set_theta_zero_location('N')         #设置极轴方向
    plt.yticks([])
    #plt.xticks([])
    ax.set_title('class {}'.format(j),fontsize = 20, y=-.2)   #添加图描述
    plt.show()
for i in range(500):
    DrawStarCoordinates(i*15)

#def DrawChernoffFaces(j):
import matplotlib
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from numpy.random import rand
from numpy import pi, arctan

def cface(ax, x1,x2,x3,x4,x6,x8,x10,x11,x13,x14):
    # x1 = height  of upper face
    # x2 = overlap of lower face
    # x3 = half of vertical size of face
    # x4 = width of upper face
    # x5 = width of lower face
    # x6 = length of nose
    # x7 = vertical position of mouth
    # x8 = curvature of mouth
    # x9 = width of mouth
    # x10 = vertical position of eyes
    # x11 = separation of eyes
    # x12 = slant of eyes
    # x13 = eccentricity of eyes
    # x14 = size of eyes
    # x15 = position of pupils
    # x16 = vertical position of eyebrows
    # x17 = slant of eyebrows
    # x18 = size of eyebrows
    
    # transform some values so that input between 0,1 yields variety of output
    
    x1 = 1.5*x1
    x2 = x1
    x3 = 1.9*(x3-.5)
    x4 = (x4+.25)*1.25
    x5 = x4*.5
    x6 = .3*(x6+.01)
    x9 = x1
    x7 = 0.5
    x8 = 15*(x8+.001)
    
    x11 /= 5
    x12 = 5
    x13 += .15
    x14 *= 1.2
    x15 = 0
    x16 = 0.25
    x17 = 0
    x18 = 0.15

    # top of face, in box with l=-x4, r=x4, t=x1, b=x3
    e = matplotlib.patches.Ellipse( (0,(x1+x3)/2), 2*x4, (x1-x3), fc='white', edgecolor='black', linewidth=2)
    # e.set_clip_box(ax.bbox)
    # e.set_facecolor([0,0,0])
    ax.add_artist(e)

    # bottom of face, in box with l=-x5, r=x5, b=-x1, t=x2+x3
    e = matplotlib.patches.Ellipse( (0,(-x1+x2+x3)/2), 2*x5, (x1+x2+x3), fc='white', edgecolor='black', linewidth=2)
    ax.add_artist(e)

    # cover overlaps
    e = matplotlib.patches.Ellipse( (0,(x1+x3)/2), 2*x4, (x1-x3), fc='white', edgecolor='black', ec='none')
    ax.add_artist(e)
    e = matplotlib.patches.Ellipse( (0,(-x1+x2+x3)/2), 2*x5, (x1+x2+x3), fc='white', edgecolor='black', ec='none')
    ax.add_artist(e)
    
    # draw nose
    ax.plot([0,0], [-x6/2, x6/2], 'k')
    
    # draw mouth
    p = matplotlib.patches.Arc( (0,-x7+.5/x8), 1/x8, 1/x8, theta1=270-180/pi*arctan(x8*x9), theta2=270+180/pi*arctan(x8*x9))
    ax.add_artist(p)
    
    # draw eyes
    p = matplotlib.patches.Ellipse( (-x11-x14/2,x10), x14, x13*x14, angle=-180/pi*x12, facecolor='white', edgecolor='black')
    ax.add_artist(p)
    
    p = matplotlib.patches.Ellipse( (x11+x14/2,x10), x14, x13*x14, angle=180/pi*x12, facecolor='white', edgecolor='black')
    ax.add_artist(p)
    
    # draw eyebrows
    ax.plot([-x11-x14/2-x14*x18/2,-x11-x14/2+x14*x18/2],[x10+x13*x14*(x16+x17),x10+x13*x14*(x16-x17)],'k')
    ax.plot([x11+x14/2+x14*x18/2,x11+x14/2-x14*x18/2],[x10+x13*x14*(x16+x17),x10+x13*x14*(x16-x17)],'k')

min_max_scaler = MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(train_data)

fig = plt.figure(figsize=(21,21))
for i in range(500):
    ax = fig.add_subplot(25,20,i+1,aspect='equal')
    
    cface(ax,  *X_train_minmax[i*15])#*rand(9)
    #cface(ax, .9, *rand(9))
    ax.axis([-1.2,1.2,-1.2,1.2])
    ax.set_xticks([])
    ax.set_yticks([])

fig.subplots_adjust(hspace=0, wspace=0)
plt.show()

"""### PCA"""

import matplotlib
#matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import pylab
def pca(X=np.array([]), no_dims=50):
    """
        Runs PCA on the NxD array X in order to reduce its dimensionality to
        no_dims dimensions.
    """

    print("Preprocessing the data using PCA...")
    (n, d) = X.shape
    X = X - np.tile(np.mean(X, 0), (n, 1))
    (l, M) = np.linalg.eig(np.dot(X.T, X))
    Y = np.dot(X, M[:, 0:no_dims])
    return Y

Y = pca(train_data, 2)

print(Y)
plt.figure(figsize=(20,20))
plt.plot(Y[A_log, 0], Y[A_log, 1], 'o', label='A')
plt.plot(Y[B_log, 0], Y[B_log, 1], 'o', label='B')
plt.plot(Y[C_log, 0], Y[C_log, 1], 'o', label='C')
plt.plot(Y[D_log, 0], Y[D_log, 1], 'o', label='D')
plt.plot(Y[E_log, 0], Y[E_log, 1], 'o', label='E')
plt.plot(Y[F_log, 0], Y[F_log, 1], 'o', label='F')
plt.plot(Y[G_log, 0], Y[G_log, 1], 'o', label='G')
plt.plot(Y[H_log, 0], Y[H_log, 1], 'o', label='H')
plt.plot(Y[I_log, 0], Y[I_log, 1], 'o', label='I')
plt.plot(Y[W_log, 0], Y[W_log, 1], 'o', label='W')
plt.plot(Y[X_log, 0], Y[X_log, 1], 'o', label='X')
plt.plot(Y[Y_log, 0], Y[Y_log, 1], 'o', label='Y')
plt.show()

#plt.plot(Y[:100], 'o')

"""### t-SNE"""

def Hbeta(D=np.array([]), beta=1.0):
    """
        Compute the perplexity and the P-row for a specific value of the
        precision of a Gaussian distribution.
    """

    # Compute P-row and corresponding perplexity
    P = np.exp(-D.copy() * beta)
    sumP = sum(P)
    H = np.log(sumP) + beta * np.sum(D * P) / sumP
    #print("sumP",sumP)
    P = P / max(sumP,1e-12)
    return H, P

def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):
    """
        Performs a binary search to get P-values in such a way that each
        conditional Gaussian has the same perplexity.
    """

    # Initialize some variables
    print("Computing pairwise distances...")
    (n, d) = X.shape
    sum_X = np.sum(np.square(X), 1)
    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)
    P = np.zeros((n, n))
    beta = np.ones((n, 1))
    logU = np.log(perplexity)

    # Loop over all datapoints
    for i in range(n):

        # Print progress
        if i % 500 == 0:
            print("Computing P-values for point %d of %d..." % (i, n))

        # Compute the Gaussian kernel and entropy for the current precision
        betamin = -np.inf
        betamax = np.inf
        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]
        (H, thisP) = Hbeta(Di, beta[i])

        # Evaluate whether the perplexity is within tolerance
        Hdiff = H - logU
        tries = 0
        while np.abs(Hdiff) > tol and tries < 50:

            # If not, increase or decrease precision
            if Hdiff > 0:
                betamin = beta[i].copy()
                if betamax == np.inf or betamax == -np.inf:
                    beta[i] = beta[i] * 2.
                else:
                    beta[i] = (beta[i] + betamax) / 2.
            else:
                betamax = beta[i].copy()
                if betamin == np.inf or betamin == -np.inf:
                    beta[i] = beta[i] / 2.
                else:
                    beta[i] = (beta[i] + betamin) / 2.

            # Recompute the values
            (H, thisP) = Hbeta(Di, beta[i])
            Hdiff = H - logU
            tries += 1

        # Set the final row of P
        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP

    # Return final P-matrix
    print("Mean value of sigma: %f" % np.mean(np.sqrt(1 / beta)))
    return P

def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):
    """
        Runs t-SNE on the dataset in the NxD array X to reduce its
        dimensionality to no_dims dimensions. The syntaxis of the function is
        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.
    """

    # Check inputs
    if isinstance(no_dims, float):
        print("Error: array X should have type float.")
        return -1
    if round(no_dims) != no_dims:
        print("Error: number of dimensions should be an integer.")
        return -1

    # Initialize variables
    X = pca(X, initial_dims).real
    (n, d) = X.shape
    max_iter = 1500
    initial_momentum = 0.5
    final_momentum = 0.8
    eta = 500
    min_gain = 0.01
    Y = np.random.randn(n, no_dims)
    dY = np.zeros((n, no_dims))
    iY = np.zeros((n, no_dims))
    gains = np.ones((n, no_dims))

    # Compute P-values
    P = x2p(X, 1e-5, perplexity)
    P = P + np.transpose(P)
    P = P / np.sum(P)
    P = P * 4.									# early exaggeration
    P = np.maximum(P, 1e-12)

    # Run iterations
    for iter in range(max_iter):

        # Compute pairwise affinities
        sum_Y = np.sum(np.square(Y), 1)
        num = -2. * np.dot(Y, Y.T)
        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))
        num[range(n), range(n)] = 0.
        Q = num / np.sum(num)
        Q = np.maximum(Q, 1e-12)

        # Compute gradient
        PQ = P - Q
        for i in range(n):
            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)

        # Perform the update
        if iter < 20:
            momentum = initial_momentum
        else:
            momentum = final_momentum
        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \
                (gains * 0.8) * ((dY > 0.) == (iY > 0.))
        gains[gains < min_gain] = min_gain
        iY = momentum * iY - eta * (gains * dY)
        Y = Y + iY
        Y = Y - np.tile(np.mean(Y, 0), (n, 1))

        # Compute current value of cost function
        if (iter + 1) % 10 == 0:
            C = np.sum(P * np.log(P / Q))
            print("Iteration %d: error is %f" % (iter + 1, C))

        # Stop lying about P-values
        if iter == 100:
            P = P / 4.

    # Return solution
    return Y

print(train_data.shape)

print("Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on dataset AVILA.")
print("Running example on AVILA digits...")
Y = tsne(train_data, 2, 10, 20.0)

print(Y)
plt.figure(figsize=(20,20))
plt.plot(Y[A_log, 0], Y[A_log, 1], 'o', label='A')
plt.plot(Y[B_log, 0], Y[B_log, 1], 'o', label='B')
plt.plot(Y[C_log, 0], Y[C_log, 1], 'o', label='C')
plt.plot(Y[D_log, 0], Y[D_log, 1], 'o', label='D')
plt.plot(Y[E_log, 0], Y[E_log, 1], 'o', label='E')
plt.plot(Y[F_log, 0], Y[F_log, 1], 'o', label='F')
plt.plot(Y[G_log, 0], Y[G_log, 1], 'o', label='G')
plt.plot(Y[H_log, 0], Y[H_log, 1], 'o', label='H')
plt.plot(Y[I_log, 0], Y[I_log, 1], 'o', label='I')
plt.plot(Y[W_log, 0], Y[W_log, 1], 'o', label='W')
plt.plot(Y[X_log, 0], Y[X_log, 1], 'o', label='X')
plt.plot(Y[Y_log, 0], Y[Y_log, 1], 'o', label='Y')

plt.show()

!nvidia-smi



